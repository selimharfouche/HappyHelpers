name: Ransomware Tracker Scraping

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 1  # Shallow clone for speed

      # Use pre-installed Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'  # Use action's built-in caching

      # Install Tor (not pre-installed, so we need this)
      - name: Install and start Tor
        run: |
          sudo apt-get update && sudo apt-get install -y tor
          sudo service tor start
          # Wait for Tor to start
          timeout=10
          elapsed=0
          while [ $elapsed -lt $timeout ]; do
            if netstat -tulpn | grep 9050 > /dev/null; then
              echo "âœ… Tor is running on port 9050"
              break
            fi
            elapsed=$((elapsed+1))
            sleep 1
          done

      # Check if Firefox is pre-installed, use it if available
      - name: Check Firefox installation
        id: check-firefox
        run: |
          if which firefox > /dev/null; then
            echo "Firefox is pre-installed: $(firefox --version)"
            echo "firefox_installed=true" >> $GITHUB_OUTPUT
            echo "firefox_path=$(which firefox)" >> $GITHUB_OUTPUT
          else
            echo "Firefox not found, will install"
            echo "firefox_installed=false" >> $GITHUB_OUTPUT
          fi

      # Only install Firefox if not already available
      - name: Setup Firefox
        if: steps.check-firefox.outputs.firefox_installed != 'true'
        uses: browser-actions/setup-firefox@v1
        with:
          firefox-version: 'latest'

      # Setup Geckodriver (required for Selenium)
      - name: Setup Geckodriver
        uses: browser-actions/setup-geckodriver@latest
        with:
          token: ${{ github.token }}

      # Install Python dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install selenium beautifulsoup4 requests
          fi

      # Create configuration files with 10 second minimum wait time
      - name: Configure for GitHub Actions
        run: |
          mkdir -p /tmp/github_configs/code
          
          # Get Firefox path
          FIREFOX_PATH=${FIREFOX_PATH:-$(which firefox)}
          
          # Create all config files
          cat > /tmp/github_configs/code/scraping_config.json << 'EOF'
          {
            "snapshots": {
              "save_html": false,
              "max_snapshots_per_site": 5,
              "cleanup_old_snapshots": true
            },
            "scheduling": {
              "frequency_hours": 6,
              "randomize_start_time": true
            }
          }
          EOF
          
          cat > /tmp/github_configs/code/proxy_config.json << 'EOF'
          {
            "proxy": {
              "type": "socks",
              "host": "127.0.0.1",
              "port": 9050,
              "remote_dns": true
            },
            "tor": {
              "auto_start": false
            }
          }
          EOF
          
          # Create browser config with 10 second minimum wait time
          cat > /tmp/github_configs/code/browser_config.json << EOF
          {
            "timing": {
              "min_wait_time": 10,
              "max_wait_time": 20,
              "tor_check_wait_time": 3,
              "page_load_timeout": 120
            },
            "anti_bot": {
              "enabled": true,
              "randomize_timing": true
            },
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; rv:102.0) Gecko/20100101 Firefox/102.0",
            "firefox_binary": "${FIREFOX_PATH}"
          }
          EOF
      
      # Prepare directories
      - name: Create data directories
        run: mkdir -p data/output data/html_snapshots data/processed

      # Run scraper
      - name: Run scraper and process entities
        run: |
          export GITHUB_CONFIG_PATH="/tmp/github_configs"
          
          # Run the scraper
          cd tracker
          python main.py
          
          # Process entities
          cd processing
          python process_entities.py
          echo "Entity processing completed"

      # Commit changes
      - name: Commit and push if there are changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git add -A data/
          
          if ! git diff --staged --quiet; then
            git commit -m "Auto-update data from scheduled scrape"
            git push
          else
            echo "No changes to commit"
          fi