name: Ransomware Tracker Scraping

on:
  schedule:
    - cron: '*/5 * * * *'  # Run every 5 minutes
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    outputs:
      found_new_entities: ${{ steps.check_new_entities.outputs.found_new_entities }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 1  # Shallow clone for speed

      # Use pre-installed Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'  # Use action's built-in caching

      # Install Tor (not pre-installed, so we need this)
      - name: Install and start Tor
        run: |
          sudo apt-get update && sudo apt-get install -y tor
          sudo service tor start
          # Wait for Tor to start
          timeout=10
          elapsed=0
          while [ $elapsed -lt $timeout ]; do
            if netstat -tulpn | grep 9050 > /dev/null; then
              echo "âœ… Tor is running on port 9050"
              break
            fi
            elapsed=$((elapsed+1))
            sleep 1
          done

      # Check if Firefox is pre-installed, use it if available
      - name: Check Firefox installation
        id: check-firefox
        run: |
          if which firefox > /dev/null; then
            echo "Firefox is pre-installed: $(firefox --version)"
            echo "firefox_installed=true" >> $GITHUB_OUTPUT
            echo "firefox_path=$(which firefox)" >> $GITHUB_OUTPUT
          else
            echo "Firefox not found, will install"
            echo "firefox_installed=false" >> $GITHUB_OUTPUT
          fi

      # Only install Firefox if not already available
      - name: Setup Firefox
        if: steps.check-firefox.outputs.firefox_installed != 'true'
        uses: browser-actions/setup-firefox@v1
        with:
          firefox-version: 'latest'

      # Setup Geckodriver (required for Selenium)
      - name: Setup Geckodriver
        uses: browser-actions/setup-geckodriver@latest
        with:
          token: ${{ github.token }}

      # Install Python dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install selenium beautifulsoup4 requests
          fi
          
          # Make sure requests is installed (needed for Telegram notifications)
          pip install requests

      # Prepare directories with updated structure
      - name: Create data directories
        run: |
          mkdir -p data/output data/snapshots/html_snapshots data/snapshots/new_entities_snapshot logs

      # Count snapshot files before scraping
      - name: Count initial snapshot files
        id: initial_count
        run: |
          SNAPSHOT_DIR="data/snapshots/new_entities_snapshot"
          mkdir -p "$SNAPSHOT_DIR"
          
          # Count initial files
          INITIAL_COUNT=$(find "$SNAPSHOT_DIR" -name "new_entities_*.json" -type f | wc -l)
          echo "initial_count=$INITIAL_COUNT" >> $GITHUB_OUTPUT
          echo "Initial count of snapshot files: $INITIAL_COUNT"

      # Run scraper with Telegram environmental variables and constant-monitoring flag
      - name: Run scraper and process entities
        env:
          # Pass GitHub secrets to the environment for Telegram notifications
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHANNEL_ID: ${{ secrets.TELEGRAM_CHANNEL_ID }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          export GITHUB_CONFIG_PATH="/tmp/github_configs"
          
          # Run the scraper with constant-monitoring flag
          cd tracker
          python main.py --constant-monitoring
          echo "Scraper and entity processing completed"

      # Count snapshot files after scraping and check if new files were created
      - name: Check for new entities
        id: check_new_entities
        run: |
          SNAPSHOT_DIR="data/snapshots/new_entities_snapshot"
          
          # Count files after scraping
          FINAL_COUNT=$(find "$SNAPSHOT_DIR" -name "new_entities_*.json" -type f | wc -l)
          echo "Final count of snapshot files: $FINAL_COUNT"
          
          # Get the initial count from previous step
          INITIAL_COUNT=${{ steps.initial_count.outputs.initial_count }}
          echo "Initial count was: $INITIAL_COUNT"
          
          # Compare counts
          if [ "$FINAL_COUNT" -gt "$INITIAL_COUNT" ]; then
            echo "New snapshot files detected!"
            echo "found_new_entities=true" >> $GITHUB_OUTPUT
            echo "NEW_ENTITIES_FOUND=true" >> $GITHUB_ENV
          else
            echo "No new snapshot files detected."
            echo "found_new_entities=false" >> $GITHUB_OUTPUT
            echo "NEW_ENTITIES_FOUND=false" >> $GITHUB_ENV
          fi
          
          # Print the output value for verification
          echo "Output value set to: $(cat $GITHUB_OUTPUT)"

      # Commit changes
      - name: Commit and push if there are changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git add -A data/
          
          if ! git diff --staged --quiet; then
            git commit -m "Auto-update data from scheduled scrape"
            git push
          else
            echo "No changes to commit"
          fi
      
      # Add a debug step
      - name: Debug output value
        run: |
          echo "Found new entities value: ${{ steps.check_new_entities.outputs.found_new_entities }}"
          echo "Initial snapshot count: ${{ steps.initial_count.outputs.initial_count }}"
          echo "This should be 'true' if new snapshots were created, 'false' otherwise"
      
      # Only dispatch if new entities were found
      - name: Trigger AI workflow
        if: steps.check_new_entities.outputs.found_new_entities == 'true'
        uses: peter-evans/repository-dispatch@v2
        with:
          event-type: run-ai-processing
          token: ${{ secrets.GITHUB_TOKEN }}